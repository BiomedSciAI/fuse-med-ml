{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nibabel as nib\n",
    "# import SimpleITK as sitk\n",
    "\n",
    "# img_filename = \"/projects/msieve/MedicalSieve/PatientData/KNIGHT/knight/data/case_00000\"\n",
    "# img = nib.load(img_filename+\"/imaging.nii.gz\")\n",
    "# img2 = sitk.ReadImage(img_filename+\"/imaging.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29044/499496885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageSlicer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'img'"
     ]
    }
   ],
   "source": [
    "img.ImageSlicer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/msieve_dev3/usr/il018850/tools/miniconda3/envs/fuse2_pub/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from fuse.data.datasets.dataset_default import DatasetDefault\n",
    "from fuse.data.datasets.caching.samples_cacher import SamplesCacher\n",
    "from fuse.data import PipelineDefault, OpSampleAndRepeat, OpToTensor, OpRepeat\n",
    "from fuse.data.ops.op_base import OpBase\n",
    "from fuse.data.ops.ops_aug_common import OpSample\n",
    "from fuse.data.ops.ops_common import OpLambda\n",
    "from fuseimg.data.ops.aug.color import OpAugColor\n",
    "from fuseimg.data.ops.aug.geometry import OpAugAffine2D\n",
    "from fuseimg.data.ops.image_loader import OpLoadImage \n",
    "from fuseimg.data.ops.color import OpClip, OpToRange\n",
    "import numpy as np\n",
    "from fuse.data.utils.sample import get_sample_id\n",
    "from typing import Hashable, List, Optional, Sequence, Tuple, Union\n",
    "from functools import partial\n",
    "from fuse.utils.ndict import NDict\n",
    "import os\n",
    "from fuse.utils.rand.param_sampler import Uniform, RandInt, RandBool\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from dataset import OpPrepare_Clinical, OpKnightSampleIDDecode, aug_op_random_crop_and_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/projects/msieve/MedicalSieve/PatientData/KNIGHT/knight/data\"\n",
    "static_pipeline = PipelineDefault(\"static\", [\n",
    "        # decoding sample ID\n",
    "        (OpKnightSampleIDDecode(), dict()), # will save image and seg path to \"data.input.img_path\", \"data.gt.seg_path\" \n",
    "        \n",
    "        # loading data\n",
    "        (OpLoadImage(data_path), dict(key_in=\"data.input.img_path\", key_out=\"data.input.img\", format=\"nib\")),\n",
    "        # (OpLoadImage(data_dir), dict(key_in=\"data.gt.seg_path\", key_out=\"data.gt.seg\", format=\"nib\")),\n",
    "        \n",
    "        \n",
    "        # fixed image normalization\n",
    "        (OpClip(), dict(key=\"data.input.img\", clip=(-62, 301))),\n",
    "        (OpLambda(lambda x: (x - 104.0)/75.3 ), dict(key=\"data.input.img\")), #kits normalization\n",
    "        # (OpToRange(), dict(key=\"data.input.img\", from_range=(-500, 500), to_range=(0, 1))),\n",
    "        \n",
    "        # transposing so the depth channel will be first\n",
    "        (OpLambda(lambda x: np.moveaxis(x, -1, 0)), dict(key=\"data.input.img\")), # convert image from shape [H, W, D] to shape [D, H, W] \n",
    "        (OpPrepare_Clinical(), dict()), #process clinical data\n",
    "\n",
    "    ])\n",
    "\n",
    "dynamic_pipeline = PipelineDefault(\"dynamic\", [\n",
    "            \n",
    "    # resize image to (110, 256, 256)\n",
    "    # (OpLambda(func=partial(my_resize, resize_to=(110, 256, 256))), dict(key=\"data.input.img\")),\n",
    "\n",
    "    # Numpy to tensor\n",
    "    (OpToTensor(), dict(key=\"data.input.img\")),\n",
    "    \n",
    "    # affine transformation per slice but with the same arguments\n",
    "    # (OpAugAffine2D() , dict(\n",
    "    #     rotate=Uniform(-180.0,180.0),        \n",
    "    #     scale=Uniform(0.8, 1.2),\n",
    "    #     flip=(RandBool(0.5), RandBool(0.5)),\n",
    "    #     translate=(RandInt(-15, 15), RandInt(-15, 15))\n",
    "    # )),\n",
    "\n",
    "    # color augmentation - check if it is useful in CT images\n",
    "    # (OpSample(OpAugColor()), dict(\n",
    "    #     key=\"data.input.img\",\n",
    "    #     gamma=Uniform(0.8,1.2), \n",
    "    #     contrast=Uniform(0.9,1.1),\n",
    "    #     add=Uniform(-0.01, 0.01)\n",
    "    # )),\n",
    "    (OpLambda(lambda x: aug_op_random_crop_and_pad(x, (256,256,110), centralize=True)), dict(key=\"data.input.img\")),\n",
    "    # add channel dimension -> [C=1, D, H, W]\n",
    "    (OpLambda(lambda x: x.unsqueeze(dim=0)), dict(key=\"data.input.img\")),  \n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2\n",
    "sample_ids=[f\"case_{id:05d}\" for id in range(num_samples)]\n",
    "my_dataset = DatasetDefault(sample_ids=sample_ids,\n",
    "    static_pipeline=static_pipeline,\n",
    "    dynamic_pipeline=dynamic_pipeline,\n",
    ")\n",
    "my_dataset.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = my_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29855/1682850174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data.input.img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "a[\"data.input.img\"][0,0,0,0].astype(torch.float64).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"data.input.img\"][0,0,0,0].float().dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = my_dataset[0][\"data.input.clinical\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.3422469e+08, 2.8640560e+06, 6.5688070e+06, 9.2477600e+06,\n",
       "        2.7454010e+06, 9.6909200e+05, 7.6067800e+05, 5.3501600e+05,\n",
       "        4.1236800e+05, 1.8421160e+06]),\n",
       " array([-2.20451527, -1.72244356, -1.24037185, -0.75830013, -0.27622842,\n",
       "         0.20584329,  0.68791501,  1.16998672,  1.65205843,  2.13413015,\n",
       "         2.61620186]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(a.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256, 110])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cols = ['case_id', 'age_at_nephrectomy', 'body_mass_index', 'gender', 'comorbidities', \\\n",
    "                                      'smoking_history', 'radiographic_size', 'last_preop_egfr', 'aua_risk_group']\n",
    "        \n",
    "json_data = pd.read_json(\"/projects/msieve/MedicalSieve/PatientData/KNIGHT/knight/data/knight.json\")[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = 0\n",
    "for d in json_data[\"comorbidities\"]:\n",
    "    # print(d.values())\n",
    "    sum += any(x for x in d.values())\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['case_id', 'age_at_nephrectomy', 'body_mass_index', 'gender', 'comorbidities', 'smoking_history', 'radiographic_size', 'last_preop_egfr'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = json_data[json_data[\"case_id\"]==\"case_00000\"].to_dict(\"records\")[0]\n",
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{17.0,\n",
       " 38.0,\n",
       " 41.0,\n",
       " 42.0,\n",
       " 44.0,\n",
       " 45.0,\n",
       " 46.0,\n",
       " 47.0,\n",
       " 48.0,\n",
       " 49.0,\n",
       " 50.0,\n",
       " 51.0,\n",
       " 52.0,\n",
       " 53.0,\n",
       " 54.0,\n",
       " 55.0,\n",
       " 56.0,\n",
       " 57.0,\n",
       " 58.0,\n",
       " 59.0,\n",
       " 60.0,\n",
       " 61.0,\n",
       " 62.0,\n",
       " 63.0,\n",
       " 64.0,\n",
       " 65.0,\n",
       " 66.0,\n",
       " 67.0,\n",
       " 68.0,\n",
       " 69.0,\n",
       " 70.0,\n",
       " 71.0,\n",
       " 72.0,\n",
       " 73.0,\n",
       " 74.0,\n",
       " 75.0,\n",
       " 76.0,\n",
       " 77.0,\n",
       " 78.0,\n",
       " 79.0,\n",
       " 80.0,\n",
       " 81.0,\n",
       " 82.0,\n",
       " 83.0,\n",
       " 84.0,\n",
       " 85.0,\n",
       " 86.0,\n",
       " 87.0,\n",
       " 88.0,\n",
       " 89.0,\n",
       " 90.0,\n",
       " '>=90',\n",
       " None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq = set()\n",
    "for d in json_data[\"last_preop_egfr\"]:\n",
    "    uniq.add(d[\"value\"])\n",
    "uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a80fd1348de351f22117b45eaff1802e43b0f1993b500d1bd2071db33c5024dd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('fuse_pub')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
