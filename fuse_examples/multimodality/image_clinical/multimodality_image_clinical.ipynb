{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On: FuseMedML and MultiModality\n",
    "\n",
    "Welcome!\n",
    "\n",
    "This notebook will guide you through the hands-on session.\n",
    "\n",
    "Open and run this notebook in Google Colab (instructions can be found in 'Installation Details - Google Colab' section):\n",
    "\n",
    "https://colab.research.google.com/github/IBM/fuse-med-ml/blob/master/fuse_examples/multimodality/image_clinical/multimodality_image_clinical.ipynb\n",
    "\n",
    "## Session take-away\n",
    "* Introduction to FuseMedML framework\n",
    "* Introduction to multi-modality data and tasks\n",
    "* Train multimodality based deep-learning model: demonstration of the integration of imaging and clinical data in skin lesion classification task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "## FuseMedML\n",
    "See https://github.com/IBM/fuse-med-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## Multimodality\n",
    "Radiologists never diagnose based solely on a single modality. The decision is made by combining information from various sources. Therefore, it is important to include such information in machine learning algorithms. \n",
    "\n",
    "Radiologists take into account clinical information such as the reason the scan was ordered. If needed, they can also examine other clinical information from the electronic health records of the hospital.  \n",
    "\n",
    "Prior images are another type of data that is routinely used in radiology reading. Radiologists will often compare a current study with imaging or other tests done in the past to assess change.\n",
    "\n",
    "It is also a common practice to consider findings from several imaging modalities when making a diagnosis. Each reveals different aspects and attributes of the suspicious finding.\n",
    "\n",
    "In this session, we will demonstrate two simple yet effective methods to integrate clinical data.\n",
    "In all cases and in general, the clinical data should first be pre-processed, normalized, etc.\n",
    "\n",
    "<img src=\"arch.png\" alt=\"drawing\" width=\"100%\"/>\n",
    "\n",
    "* **Imaging only implementation**\n",
    "\n",
    "* **Imaging and Tabular data - concatenate tabular data after image feature extraction**\n",
    "\n",
    "    The tabular data is integrated after feature extraction, done by a convolutional network followed by a pooling layer that extracts non-spatial features from the image.\n",
    "\n",
    "\n",
    "* **Imaging and Tabular data - concatenate directly with the image**\n",
    "\n",
    "    The tabular data is integrated at the beginning of the network by adding more channels to the input image. Each channel represents a single bit in the one-hot vector. \n",
    "\n",
    "    This method of integrating clinical features provides the network with the ability to extract better features using the backbone, in contrast to the standard way of integrating this data only after feature extraction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Task - ISIC 2019 challenge to classify dermoscopic images and clinical data among nine different diagnostic categories.\n",
    "\n",
    "This task was chosen for demonstration since the data is simple and public, which will make the session more effective.\n",
    "\n",
    "We explored the effectiveness of each method in two different tasks:\n",
    "* Article: [Context in medical imaging: the case of focal liver lesion classification](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12032/120320O/Context-in-medical-imaging--the-case-of-focal-liver/10.1117/12.2609385.short?SSO=1)\n",
    "* FuseMedML example on [Duke dataset](https://sites.duke.edu/mazurowski/resources/breast-cancer-mri-dataset/).\n",
    "\n",
    "Skin cancer is the most common cancer globally, with melanoma being the most deadly form. \n",
    "\n",
    "Dermoscopy is a skin imaging modality that has demonstrated improvement for the diagnosis of skin cancer compared to unaided visual inspection. \n",
    "\n",
    "The goal for ISIC 2019 is to classify dermoscopic images among eight different diagnostic categories:\n",
    "\n",
    "* Melanoma\n",
    "* Melanocytic nevus\n",
    "* Basal cell carcinoma\n",
    "* Actinic keratosis\n",
    "* Benign keratosis (solar lentigo / seborrheic keratosis / lichen planus-like keratosis)\n",
    "* Dermatofibroma\n",
    "* Vascular lesion\n",
    "* Squamous cell carcinoma\n",
    "* None of the others\n",
    "\n",
    "25,331 images are available for training across 8 different categories.\n",
    "\n",
    "Two tasks were available for participation:\n",
    "* classify dermoscopic images without meta-data,\n",
    "* classify images with additional available meta-data, including age, gender and anatomic site\n",
    "\n",
    "[1] Tschandl P., Rosendahl C. & Kittler H. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Sci. Data 5, 180161 doi.10.1038/sdata.2018.161 (2018)\n",
    "\n",
    "[2] Noel C. F. Codella, David Gutman, M. Emre Celebi, Brian Helba, Michael A. Marchetti, Stephen W. Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, Allan Halpern: “Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC)”, 2017; arXiv:1710.05006.\n",
    "\n",
    "[3] Marc Combalia, Noel C. F. Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer Reiter, Allan C. Halpern, Susana Puig, Josep Malvehy: “BCN20000: Dermoscopic Lesions in the Wild”, 2019; arXiv:1908.02288."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Installation Details - Google Colab (skip if you run it using an already installed FuseMedML)\n",
    "\n",
    "### **Enable GPU Support**\n",
    "\n",
    "To use GPU through Google Colab, change the runtime mode to GPU:\n",
    "\n",
    "From the \"Runtime\" menu select \"Change Runtime Type\", choose \"GPU\" from the drop-down menu and click \"SAVE\"\n",
    "When asked, reboot the system.\n",
    "\n",
    "### **Install FuseMedML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/IBM/fuse-med-ml.git\n",
    "%cd fuse-med-ml\n",
    "!pip install -e .[all,examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please reboot the session when asked!**\n",
    "----------------\n",
    "### Select GPU, start a logger and define dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from fuse.utils.utils_logger import fuse_logger_start\n",
    "import logging\n",
    "\n",
    "fuse_logger_start(output_path=None, console_verbose_level=logging.INFO)\n",
    "all_data = False  # use all data or just 400 samples\n",
    "model_dir = \"model_dir\"  # path to model dir\n",
    "cache_dir = \"cache_dir\"  # path to cache dir\n",
    "data_dir = \"data_dir\"\n",
    "reset_cache = True\n",
    "reset_split_file = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### Data\n",
    "\n",
    "(Don't forget to first follow the installation instructions listed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Download data: it might take few miuntes\")\n",
    "from fuseimg.datasets.isic import ISIC\n",
    "\n",
    "if not all_data:\n",
    "    from fuse_examples.imaging.classification.isic.golden_members import FULL_GOLDEN_MEMBERS as sample_ids\n",
    "else:\n",
    "    sample_ids = None\n",
    "ISIC.download(data_path=data_dir, sample_ids_to_download=sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuse_examples.multimodality.image_clinical.dataset import isic_2019_dataloaders\n",
    "\n",
    "# build the dataloders (and dataset) using FuseMedML data package and processing implementation located in fuseimg.datasets.isic\n",
    "train_dl, validation_dl = isic_2019_dataloaders(\n",
    "    data_path=data_dir, cache_path=cache_dir, reset_cache=reset_cache, reset_split_file=reset_split_file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original code can be found [here](https://github.com/IBM/fuse-med-ml/blob/master/fuse_examples/multimodality/image_cliical/dataset.py).\n",
    "\n",
    "<br/>\n",
    "\n",
    "Details and instructions about our data package can be found [here](https://github.com/IBM/fuse-med-ml/blob/master/fuse/data/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 10\n",
    "print(train_dl.dataset[sample_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imaging Only Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuse.dl.models.model_multihead import ModelMultiHead\n",
    "from fuse.dl.models.heads.head_global_pooling_classifier import HeadGlobalPoolingClassifier\n",
    "from fuse.dl.models.backbones.backbone_inception_resnet_v2 import BackboneInceptionResnetV2\n",
    "\n",
    "model = ModelMultiHead(\n",
    "    conv_inputs=((\"data.input.img\", 3),),\n",
    "    backbone=BackboneInceptionResnetV2(input_channels_num=3, pretrained_weights_url=None),\n",
    "    heads=[\n",
    "        HeadGlobalPoolingClassifier(\n",
    "            head_name=\"classification\",\n",
    "            dropout_rate=0.5,\n",
    "            conv_inputs=[(\"model.backbone_features\", 384)],\n",
    "            layers_description=(256,),\n",
    "            num_classes=8,\n",
    "            pooling=\"avg\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "from fuse.dl.losses.loss_default import LossDefault\n",
    "from fuse.eval.metrics.classification.metrics_classification_common import MetricAUCROC, MetricAccuracy, MetricConfusion\n",
    "from fuse.eval.metrics.classification.metrics_thresholding_common import MetricApplyThresholds\n",
    "\n",
    "# ====================================================================================\n",
    "#  Loss\n",
    "# ====================================================================================\n",
    "losses = {\n",
    "    \"cls_loss\": LossDefault(\n",
    "        pred=\"model.logits.classification\", target=\"data.label\", callable=F.cross_entropy, weight=1.0\n",
    "    )\n",
    "}\n",
    "\n",
    "# ====================================================================================\n",
    "# Metrics\n",
    "# ====================================================================================\n",
    "class_names = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\"]\n",
    "\n",
    "train_metrics = OrderedDict(\n",
    "    [\n",
    "        (\"op\", MetricApplyThresholds(pred=\"model.output.classification\")),  # will apply argmax,\n",
    "        (\n",
    "            \"balanced_acc\",\n",
    "            MetricConfusion(\n",
    "                pred=\"results:metrics.op.cls_pred\",\n",
    "                target=\"data.label\",\n",
    "                metrics=(\"sensitivity\",),\n",
    "                class_names=class_names,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "validation_metrics = copy.deepcopy(train_metrics)\n",
    "\n",
    "best_epoch_source = {\n",
    "    \"monitor\": \"validation.metrics.balanced_acc.sensitivity.macro_avg\",\n",
    "    \"mode\": \"max\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from fuse.dl.lightning.pl_module import LightningModuleDefault\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.001)\n",
    "\n",
    "# create scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "lr_sch_config = dict(scheduler=lr_scheduler, monitor=\"validation.losses.total_loss\")\n",
    "# optimizier and lr sch - see pl.LightningModule.configure_optimizers return value for all options\n",
    "optimizers_and_lr_schs = dict(optimizer=optimizer, lr_scheduler=lr_sch_config)\n",
    "\n",
    "# create instance of PL module - FuseMedML generic version\n",
    "pl_module = LightningModuleDefault(\n",
    "    model_dir=model_dir,\n",
    "    model=model,\n",
    "    losses=losses,\n",
    "    train_metrics=train_metrics,\n",
    "    validation_metrics=validation_metrics,\n",
    "    best_epoch_source=best_epoch_source,\n",
    "    optimizers_and_lr_schs=optimizers_and_lr_schs,\n",
    ")\n",
    "\n",
    "# create lightining trainer.\n",
    "pl_trainer = pl.Trainer(default_root_dir=model_dir, max_epochs=2, accelerator=\"gpu\", devices=1)\n",
    "\n",
    "# train\n",
    "pl_trainer.fit(pl_module, train_dl, validation_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension - model_dir is hard-coded (change it if necessary)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imaging and Tabular data - concatenate tabular data after image feature extraction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuse.utils.ndict import NDict\n",
    "import torch\n",
    "from fuse.data import OpBase\n",
    "from fuse_examples.multimodality.image_clinical.dataset import ANATOM_SITE_INDEX, SEX_INDEX\n",
    "\n",
    "### Generate Data\n",
    "# Create an operation to add to the end of the current processing pipeline\n",
    "class OpClinicalEncodeing(OpBase):\n",
    "    \"\"\"\n",
    "    Collect clinical data into a single vector and stored it in sample_dict[\"data.input.clinical.all\"]\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample_dict: NDict) -> NDict:\n",
    "\n",
    "        age = sample_dict[\"data.input.clinical.age_approx\"]\n",
    "        if age > 0 and age < 120:\n",
    "            age = torch.tensor(age / 120.0).reshape(-1)\n",
    "        else:\n",
    "            age = torch.tensor(-1.0).reshape(-1)\n",
    "\n",
    "        anatom_site = sample_dict[\"data.input.clinical.anatom_site_general\"]\n",
    "        anatom_site_one_hot = torch.zeros(len(ANATOM_SITE_INDEX))\n",
    "        if anatom_site in ANATOM_SITE_INDEX:\n",
    "            anatom_site_one_hot[ANATOM_SITE_INDEX[anatom_site]] = 1\n",
    "\n",
    "        sex = sample_dict[\"data.input.clinical.sex\"]\n",
    "        sex_one_hot = torch.zeros(len(SEX_INDEX))\n",
    "        if sex in SEX_INDEX:\n",
    "            sex_one_hot[SEX_INDEX[sex]] = 1\n",
    "\n",
    "        clinical_encoding = torch.cat((age, anatom_site_one_hot, sex_one_hot), dim=0)\n",
    "        sample_dict[\"data.input.clinical.all\"] = clinical_encoding\n",
    "\n",
    "        return sample_dict\n",
    "\n",
    "\n",
    "# add single step to the pipeline. format (<operation>, kwargs for <operation>.__call__() method)\n",
    "append_dyn_pipeline = [(OpClinicalEncodeing(), dict())]\n",
    "train_dl, validation_dl = isic_2019_dataloaders(\n",
    "    data_path=data_dir, cache_path=cache_dir, append_dyn_pipeline=append_dyn_pipeline, sample_ids=sample_ids\n",
    ")\n",
    "\n",
    "### Define model - add the new clinical data vector as an additional argument to the classification head\n",
    "model = ModelMultiHead(\n",
    "    conv_inputs=((\"data.input.img\", 3),),\n",
    "    backbone=BackboneInceptionResnetV2(input_channels_num=3, pretrained_weights_url=None),\n",
    "    heads=[\n",
    "        HeadGlobalPoolingClassifier(\n",
    "            head_name=\"classification\",\n",
    "            dropout_rate=0.5,\n",
    "            conv_inputs=[(\"model.backbone_features\", 384)],\n",
    "            tabular_data_inputs=[(\"data.input.clinical.all\", 13)],\n",
    "            layers_description=(256,),\n",
    "            tabular_layers_description=(128,),\n",
    "            num_classes=8,\n",
    "            pooling=\"avg\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.001)\n",
    "\n",
    "# create scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "lr_sch_config = dict(scheduler=lr_scheduler, monitor=\"validation.losses.total_loss\")\n",
    "# optimizier and lr sch - see pl.LightningModule.configure_optimizers return value for all options\n",
    "optimizers_and_lr_schs = dict(optimizer=optimizer, lr_scheduler=lr_sch_config)\n",
    "\n",
    "### Strart a training process\n",
    "# create instance of PL module - FuseMedML generic version\n",
    "pl_module = LightningModuleDefault(\n",
    "    model_dir=model_dir,\n",
    "    model=model,\n",
    "    losses=losses,\n",
    "    train_metrics=train_metrics,\n",
    "    validation_metrics=validation_metrics,\n",
    "    best_epoch_source=best_epoch_source,\n",
    "    optimizers_and_lr_schs=optimizers_and_lr_schs,\n",
    ")\n",
    "\n",
    "# create lightining trainer.\n",
    "pl_trainer = pl.Trainer(default_root_dir=model_dir, max_epochs=2, accelerator=\"gpu\", devices=1)\n",
    "\n",
    "\n",
    "# train\n",
    "pl_trainer.fit(pl_module, train_dl, validation_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imaging and Tabular data - concatenate directly with the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate Data\n",
    "# Create an operation to add to the end of the current processing pipeline\n",
    "class OpClinicalPadToImage(OpBase):\n",
    "    \"\"\"append the clinical data directly with the image\"\"\"\n",
    "\n",
    "    def __call__(self, sample_dict: NDict) -> NDict:\n",
    "        clinical_encoding = sample_dict[\"data.input.clinical.all\"]\n",
    "        image = sample_dict[\"data.input.img\"]\n",
    "\n",
    "        clinical_data_spatial = clinical_encoding.reshape((clinical_encoding.shape + (1, 1))).repeat(\n",
    "            (1,) + image.shape[1:]\n",
    "        )  # repeat and reshape to [num_features, H, W]\n",
    "        image = torch.cat((image, clinical_data_spatial), dim=0)  # concat to get [num_features + 3, H, W]\n",
    "\n",
    "        sample_dict[\"data.input.img\"] = image\n",
    "\n",
    "        return sample_dict\n",
    "\n",
    "\n",
    "# add two steps to the pipeline. format (<operation>, kwargs for <operation>.__call__() method)\n",
    "append_dyn_pipeline = [(OpClinicalEncodeing(), dict()), (OpClinicalPadToImage(), dict())]\n",
    "train_dl, validation_dl = isic_2019_dataloaders(\n",
    "    data_path=data_dir, cache_path=cache_dir, append_dyn_pipeline=append_dyn_pipeline, sample_ids=sample_ids\n",
    ")\n",
    "\n",
    "### Define model - this time add image with 16 channels (with the clinical data embbeded into the image)\n",
    "model = ModelMultiHead(\n",
    "    conv_inputs=((\"data.input.img\", 16),),\n",
    "    backbone=BackboneInceptionResnetV2(input_channels_num=16, pretrained_weights_url=None),\n",
    "    heads=[\n",
    "        HeadGlobalPoolingClassifier(\n",
    "            head_name=\"classification\",\n",
    "            dropout_rate=0.5,\n",
    "            conv_inputs=[(\"model.backbone_features\", 384)],\n",
    "            layers_description=(256,),\n",
    "            num_classes=8,\n",
    "            pooling=\"avg\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=0.001)\n",
    "\n",
    "# create scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "lr_sch_config = dict(scheduler=lr_scheduler, monitor=\"validation.losses.total_loss\")\n",
    "# optimizier and lr sch - see pl.LightningModule.configure_optimizers return value for all options\n",
    "optimizers_and_lr_schs = dict(optimizer=optimizer, lr_scheduler=lr_sch_config)\n",
    "\n",
    "\n",
    "# create instance of PL module - FuseMedML generic version\n",
    "pl_module = LightningModuleDefault(\n",
    "    model_dir=\"model_dir\",\n",
    "    model=model,\n",
    "    losses=losses,\n",
    "    train_metrics=train_metrics,\n",
    "    validation_metrics=validation_metrics,\n",
    "    best_epoch_source=best_epoch_source,\n",
    "    optimizers_and_lr_schs=optimizers_and_lr_schs,\n",
    ")\n",
    "\n",
    "\n",
    "# create lightining trainer.\n",
    "pl_trainer = pl.Trainer(default_root_dir=\"model_dir\", max_epochs=2, accelerator=\"gpu\", devices=1)\n",
    "# train\n",
    "pl_trainer.fit(pl_module, train_dl, validation_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('fusedrug2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "8ea7d6d9daa35002c679b80b6cd736cc3ab69ad98231e51d0b243462a8e705f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
