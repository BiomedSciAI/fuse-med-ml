{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hands-On: FuseMedML and MultiModality\n",
    "\n",
    "Welcome!\n",
    "\n",
    "This notebook will guide you through the hands-on session.\n",
    "\n",
    "## Session take-away\n",
    "* Introduction to FuseMedML framework\n",
    "* Introduction to multi-modality data and tasks\n",
    "* Train multimodality based deep-learning model: demonstration of the integration of imaging and clinical data in skin lesion classification task\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installation Details - Google Colab\n",
    "\n",
    "### **Enable GPU Support**\n",
    "\n",
    "To use GPU through Google Colab, change the runtime mode to GPU:\n",
    "\n",
    "From the \"Runtime\" menu select \"Change Runtime Type\", choose \"GPU\" from the drop-down menu and click \"SAVE\"\n",
    "When asked, reboot the system.\n",
    "\n",
    "### **Clone hands-on session repsitory**\n",
    "\n",
    "Type `!git clone https://github.com/IBM/fuse-med-ml.git`\n",
    "\n",
    "Then, repen the notebook `fuse-med-ml/fuse_examples/tutorials/multimodality_image_clinical/multimodality_image_clinical.ipynb` in Google Colab\n",
    "\n",
    "TODO: check if we need also `pip install ipykernel --upgrad`\n",
    "\n",
    "### **Install FuseMedML**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "!pip install fuse-med-ml"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: fuse-med-ml in /projects/msieve_dev3/usr/moshikor/git_repos/fuse-med-ml (0.1.9)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (1.19.5)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (1.1.5)\n",
      "Requirement already satisfied: tqdm>=4.52.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (4.62.3)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (1.5.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.3 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (3.3.4)\n",
      "Requirement already satisfied: scikit-image>=0.17.2 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (0.17.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23.2 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (0.24.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (1.1.0)\n",
      "Requirement already satisfied: torch>=1.5.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (1.9.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (0.10.1)\n",
      "Requirement already satisfied: SimpleITK>=1.2.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (2.1.1)\n",
      "Requirement already satisfied: wget in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (3.2)\n",
      "Requirement already satisfied: tensorboard in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from fuse-med-ml) (2.6.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from matplotlib>=3.3.3->fuse-med-ml) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from matplotlib>=3.3.3->fuse-med-ml) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from matplotlib>=3.3.3->fuse-med-ml) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from matplotlib>=3.3.3->fuse-med-ml) (8.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from matplotlib>=3.3.3->fuse-med-ml) (2.8.2)\n",
      "Requirement already satisfied: six in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=3.3.3->fuse-med-ml) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from pandas>=1.1.4->fuse-med-ml) (2021.3)\n",
      "Requirement already satisfied: networkx>=2.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from scikit-image>=0.17.2->fuse-med-ml) (2.5.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from scikit-image>=0.17.2->fuse-med-ml) (2020.9.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from scikit-image>=0.17.2->fuse-med-ml) (1.1.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from scikit-image>=0.17.2->fuse-med-ml) (2.9.0)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from networkx>=2.0->scikit-image>=0.17.2->fuse-med-ml) (4.4.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from scikit-learn>=0.23.2->fuse-med-ml) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from scikit-learn>=0.23.2->fuse-med-ml) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from torch>=1.5.0->fuse-med-ml) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from torch>=1.5.0->fuse-med-ml) (0.8)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (58.0.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (3.18.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (1.35.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (2.0.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (0.37.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (0.14.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (1.41.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from tensorboard->fuse-med-ml) (2.26.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->fuse-med-ml) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->fuse-med-ml) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard->fuse-med-ml) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->fuse-med-ml) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard->fuse-med-ml) (4.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->fuse-med-ml) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->fuse-med-ml) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->fuse-med-ml) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->fuse-med-ml) (2020.12.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard->fuse-med-ml) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->fuse-med-ml) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /projects/msieve_dev3/usr/moshikor/envs_miniconda/envs/public-fuse/lib/python3.6/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->fuse-med-ml) (3.6.0)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FuseMedML\n",
    "[![Github repo](https://img.shields.io/static/v1?label=GitHub&message=FuseMedML&color=brightgreen)](https://github.com/IBM/fuse-med-ml)\n",
    "\n",
    "[![PyPI version](https://badge.fury.io/py/fuse-med-ml.svg)](https://badge.fury.io/py/fuse-med-ml)\n",
    "\n",
    "[![Slack channel](https://img.shields.io/badge/support-slack-slack.svg?logo=slack)](https://join.slack.com/t/newworkspace-i3g4445/shared_invite/zt-sr0hcb9f-E~SLYbG9bE5fn8iq5OE0ww)\n",
    "\n",
    "[![Open Source](https://badges.frapsoft.com/os/v1/open-source.svg)](https://github.com/IBM/fuse-med-ml)\n",
    "\n",
    "\n",
    "FuseMedML is an open-source python-based framework designed to enhance collaboration and accelerate discoveries in Fused Medical data through advanced Machine Learning technologies. \n",
    "\n",
    "Initial version is PyTorch-based and focuses on deep learning on medical imaging and digital pathology.\n",
    "\n",
    "### **Why use FuseMedML?**\n",
    "Successful deep learning R&D must rely on knowledge and experiments, accumulated over a wide variety of projects, and developed by different people and teams.\n",
    "\n",
    "FuseMedML is an outstanding collaboration framework that allows you to rerun an experiment or reuse some of the capabilities originally written for different projects—all with minimal effort.\\\n",
    "Using FuseMedML, you can write generic components that can be easily shared between projects in a plug & play manner, simplfying sharing and collaboration.\n",
    "\n",
    "The framework’s unique software design provides many advantages, making it an ideal framework for deep-learning research and development in medical imaging:\n",
    "\n",
    "* **Rapid development**\n",
    "\n",
    "* **Flexible, customizable, and scalable**\n",
    "\n",
    "* **Encourage sharing and collaboration** \n",
    "\n",
    "* **Collection of common, easy to use, generic components and capabilities** \n",
    "\n",
    "* **Standardized evaluation**\n",
    "\n",
    "* **Medical imaging expertise** \n",
    "\n",
    "* **Compatibility with alternative frameworks**\n",
    "\n",
    "### **FuseMedML Key Concepts in a Nutshell**\n",
    "### Decoupling\n",
    "The decoupling is achieved by the fact that, in most cases, the objects do not interact directly. Instead, the information and data are routed between components using *namespaces* (examples below). \n",
    "\n",
    "Meaning, each object extracts its input from and saves its output into a dictionary named `batch_dict`. \n",
    "\n",
    "`batch_dict` aggregates the outputs of all the objects through a single batch. \n",
    "\n",
    "When a batch is completed, only the required key-value pairs from `batch_dict`, such as the loss values, will be collected in another dictionary named `epoch_results`.\n",
    " \n",
    "\n",
    "Both `batch_dict` and `epoch_results` are nested dictionaries. To easily access the data stored in those dictionaries, use `FuseUtilsHierarchicalDict`:\n",
    "\n",
    "```python\n",
    "FuseUtilsHierarchicalDict.get(batch_dict, ‘model.output.classification’)\n",
    "``` \n",
    "\n",
    "will return `batch_dict[‘model’][‘output’][‘classification’]`\n",
    "\n",
    "**Example of the decoupling approach:**\n",
    "```python\n",
    "FuseMetricAUC(pred_name='model.output.classification', target_name='data.gt.classification')  \n",
    "```\n",
    "\n",
    "`FuseMetricAUC` will read the required tensors to compute AUC from `batch_dict`. The relevant dictionary keys are `pred_name` and `target_name`. \n",
    "\n",
    "This approach allows writing a generic metric which is completely independent of the model and data extractor. \n",
    "\n",
    "In addition, it allows to easily re-use this object in a plug & play manner without adding extra code. \n",
    "\n",
    "Such an approach also allows us to use it several times in case we have multiple heads/tasks.\n",
    "\n",
    "### Share and Reuse\n",
    "With just minimal code implementation, you can get a fully-featured pipeline up and running, form data handling, through training, inference and evaluation. \n",
    "\n",
    "Including caching, augmentation, monitoring, logging, and more. \n",
    "\n",
    "A common generic implementation, you can reuse, is provided for most components in the pipeline and all you need to do is implement specific components such as data extractors. \n",
    "\n",
    "The naming convention for the common implementation is `Fuse***Default` \n",
    "\n",
    "FuseMedML comes with a large collection of components that grow with each new project. Some of them are entirely generic and the others are domain specific.\n",
    "\n",
    "The default implementation of modules and components suits many common cases. If needed, thanks to the fact that the components in the pipeline are decoupled, you can re-implement any component to achieve the required behavior.\n",
    "\n",
    "\n",
    "Don't forget to **contribute** back and **share** them. \n",
    "\n",
    "\n",
    "### Use PyTorch directly and alternative frameworks\n",
    "\n",
    "FuseMedML uses and extends PyTorch only when required by the user. \n",
    "You can mix FuseMedML with PyTorch code, components from alternative frameworks and other popular GitHub projects. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"fusemedml-release-plans.png\" alt=\"drawing\" width=\"100%\"/>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multimodality\n",
    "Radiologists never diagnose based solely on a single modality. The decision is made by combining information from various sources. Therefore, it is important to include such information in machine learning algorithms. \n",
    "\n",
    "Radiologists take into account clinical information such as the reason the scan was ordered. If needed, they can also examine other clinical information from the electronic health records of the hospital.  \n",
    "\n",
    "Prior images are another type of data that is routinely used in radiology reading. Radiologists will often compare a current study with imaging or other tests done in the past to assess change.\n",
    "\n",
    "It is also a common practice to consider findings from several imaging modalities when making a diagnosis. Each reveals different aspects and attributes of the suspicious finding.\n",
    "\n",
    "In this session, we will demonstrate two simple yet effective methods to integrate clinical data.\n",
    "In all cases and in general, the clinical data should first be pre-processed, normalized, etc.\n",
    "\n",
    "<img src=\"arch.png\" alt=\"drawing\" width=\"70%\"/>\n",
    "\n",
    "* **Imaging only implementation**\n",
    "\n",
    "* **Imaging and Tabular data - concatenate tabular data after image feature extraction**\n",
    "\n",
    "    The tabular data is integrated after feature extraction, done by a convolutional network followed by a pooling layer that extracts non-spatial features from the image.\n",
    "\n",
    "\n",
    "* **Imaging and Tabular data - concatenate directly with the image**\n",
    "\n",
    "    The tabular data is integrated at the beginning of the network by adding more channels to the input image. Each channel represents a single bit in the one-hot vector. \n",
    "\n",
    "    This method of integrating clinical features provides the network with the ability to extract better features using the backbone, in contrast to the standard way of integrating this data only after feature extraction.\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task - ISIC 2019 challenge to classify dermoscopic images and clinical data among nine different diagnostic categories.\n",
    "\n",
    "This task was chosen for demonstration since the data is simple and public, which will make the session more effective.\n",
    "\n",
    "We explored the effectiveness of each method in two different tasks:\n",
    "* Article: \"Context in medical imaging: the case of focal liver lesion classification\" - soon to be published\n",
    "* FuseMedML example on [Duke dataset](https://sites.duke.edu/mazurowski/resources/breast-cancer-mri-dataset/) - will be published in FuseMedML repository.\n",
    "\n",
    "Skin cancer is the most common cancer globally, with melanoma being the most deadly form. \n",
    "\n",
    "Dermoscopy is a skin imaging modality that has demonstrated improvement for the diagnosis of skin cancer compared to unaided visual inspection. \n",
    "\n",
    "However, clinicians should receive adequate training for those improvements to be realized.\n",
    "\n",
    "In order to make expertise more widely available, the International Skin Imaging Collaboration (ISIC) has developed the ISIC Archive, \n",
    "\n",
    "an international repository of dermoscopic images, for both the purposes of clinical training, and for supporting technical research toward automated algorithmic analysis by hosting the ISIC Challenges.\n",
    "\n",
    "The goal for ISIC 2019 is to classify dermoscopic images among nine different diagnostic categories:\n",
    "\n",
    "* Melanoma\n",
    "* Melanocytic nevus\n",
    "* Basal cell carcinoma\n",
    "* Actinic keratosis\n",
    "* Benign keratosis (solar lentigo / seborrheic keratosis / lichen planus-like keratosis)\n",
    "* Dermatofibroma\n",
    "* Vascular lesion\n",
    "* Squamous cell carcinoma\n",
    "* None of the others\n",
    "\n",
    "25,331 images are available for training across 8 different categories.\n",
    "\n",
    "Two tasks were available for participation:\n",
    "* classify dermoscopic images without meta-data,\n",
    "* classify images with additional available meta-data, including age, gender and anatomic site"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "!export PYTHONPATH=$PYTHONPATH:$(pwd)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from fuse.utils.utils_logger import fuse_logger_start\n",
    "import logging\n",
    "fuse_logger_start(output_path=None, console_verbose_level=logging.INFO)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[1m\u001b[2m\u001b[36mFuse: 20/10/21 15:22:50\u001b[0m\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(\"It might take few miuntes\")\n",
    "\n",
    "from download import download_and_extract_isic\n",
    "download_and_extract_isic()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "It might take few miuntes\n",
      "Training Input Path: /projects/msieve_dev3/usr/moshikor/git_repos/fuse-med-ml/fuse_examples/tutorials/multimodality_image_clinical/data/ISIC2019/ISIC_2019_Training_Input\n",
      "\n",
      "Extract ISIC-2019 training input ... (this may take a few minutes)\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# explain about FuseMedML data pipeline\n",
    "from dataset import isic_2019_dataset\n",
    "\n",
    "train_dl, valid_dl = isic_2019_dataset(size=400, reset_cache=True, post_cache_processing_func=None)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "train_dl.dataset.visualize(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dl.dataset.visualize_augmentation(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(train_dl.dataset.summary([\"data.gt.gt_global.tensor\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imaging Only Implementation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from fuse.models.model_default import FuseModelDefault\n",
    "from fuse.models.heads.head_global_pooling_classifier import FuseHeadGlobalPoolingClassifier\n",
    "from fuse.models.backbones.backbone_inception_resnet_v2 import FuseBackboneInceptionResnetV2\n",
    "\n",
    "model = FuseModelDefault(\n",
    "    conv_inputs=(('data.input.image', 3),),\n",
    "    backbone=FuseBackboneInceptionResnetV2(input_channels_num=3),\n",
    "    heads=[\n",
    "        FuseHeadGlobalPoolingClassifier(head_name='head_0',\n",
    "                                        dropout_rate=0.5,\n",
    "                                        conv_inputs=[('model.backbone_features', 384)],\n",
    "                                        layers_description=(256,),\n",
    "                                        num_classes=8,\n",
    "                                        pooling=\"avg\"),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.nn.functional as F\n",
    "from fuse.losses.loss_default import FuseLossDefault\n",
    "from fuse.metrics.classification.metric_auc import FuseMetricAUC\n",
    "from fuse.metrics.classification.metric_accuracy import FuseMetricAccuracy\n",
    "from fuse.metrics.classification.metric_confusion import FuseMetricConfusion\n",
    "\n",
    "# ====================================================================================\n",
    "#  Loss\n",
    "# ====================================================================================\n",
    "losses = {\n",
    "    'cls_loss': FuseLossDefault(pred_name='model.logits.head_0', target_name='data.gt.gt_global.tensor',\n",
    "                                callable=F.cross_entropy, weight=1.0)\n",
    "}\n",
    "\n",
    "# ====================================================================================\n",
    "# Metrics\n",
    "# ====================================================================================\n",
    "metrics = {\n",
    "    'auc': FuseMetricAUC(pred_name='model.output.head_0', target_name='data.gt.gt_global.tensor'),\n",
    "    'accuracy': FuseMetricAccuracy(pred_name='model.output.head_0', target_name='data.gt.gt_global.tensor'),\n",
    "    \"balanced_acc\": FuseMetricConfusion(pred_name='model.output.head_0', target_name='data.gt.gt_global.tensor')\n",
    "}\n",
    "\n",
    "best_epoch_source = {\n",
    "    'source': 'metrics.balanced_acc.sensitivity_macro_avg',  # can be any key from losses or metrics dictionaries\n",
    "    'optimization': 'max',  # can be either min/max\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch.optim as optim\n",
    "from fuse.managers.manager_default import FuseManagerDefault\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5,\n",
    "                        weight_decay=0.001)\n",
    "\n",
    "# create scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# train from scratch\n",
    "manager = FuseManagerDefault(output_model_dir=\"model_dir\", force_reset=True)\n",
    "# Providing the objects required for the training process.\n",
    "manager.set_objects(net=model,\n",
    "                    optimizer=optimizer,\n",
    "                    losses=losses,\n",
    "                    metrics=metrics,\n",
    "                    best_epoch_source=best_epoch_source,\n",
    "                    lr_scheduler=scheduler,\n",
    "                    callbacks={},\n",
    "                    train_params={'num_epochs': 2},\n",
    "                    output_model_dir=\"model_dir\")\n",
    "\n",
    "# Start training\n",
    "manager.train(train_dataloader=train_dl,\n",
    "                validation_dataloader=valid_dl)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imaging and Tabular data - concatenate tabular data after image feature extraction ###"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from fuse.utils.utils_hierarchical_dict import FuseUtilsHierarchicalDict\n",
    "import torch\n",
    "from dataset import ANATOM_SITE_INDEX, SEX_INDEX\n",
    "\n",
    "### Generate Data\n",
    "def post_cache_processing_clinical_encoding(sample_dict: dict) -> dict:\n",
    "    age = FuseUtilsHierarchicalDict.get(sample_dict, 'data.input.clinical.age_approx')\n",
    "    if age > 0 and age < 120:\n",
    "        age = torch.tensor(age / 120.0).reshape(-1)\n",
    "    else:\n",
    "        age = torch.tensor(-1.0).reshape(-1)\n",
    "    \n",
    "    anatom_site = FuseUtilsHierarchicalDict.get(sample_dict, 'data.input.clinical.anatom_site_general')\n",
    "    anatom_site_one_hot = torch.zeros(len(ANATOM_SITE_INDEX))\n",
    "    if anatom_site in ANATOM_SITE_INDEX:\n",
    "        anatom_site_one_hot[ANATOM_SITE_INDEX[anatom_site]] = 1\n",
    "    \n",
    "    sex = FuseUtilsHierarchicalDict.get(sample_dict, 'data.input.clinical.sex')\n",
    "    sex_one_hot = torch.zeros(len(SEX_INDEX))\n",
    "    if sex in SEX_INDEX:\n",
    "        sex_one_hot[SEX_INDEX[sex]] = 1\n",
    "    \n",
    "    clinical_encoding = torch.cat((age, anatom_site_one_hot, sex_one_hot), dim=0)\n",
    "    FuseUtilsHierarchicalDict.set(sample_dict, \"data.input.clinical.all\", clinical_encoding)\n",
    "\n",
    "    return sample_dict\n",
    "\n",
    "\n",
    "train_dl, valid_dl = isic_2019_dataset(size=400, reset_cache=False, post_cache_processing_func=post_cache_processing_clinical_encoding)\n",
    "\n",
    "### Define model\n",
    "model = FuseModelDefault(\n",
    "    conv_inputs=(('data.input.image', 3),),\n",
    "    backbone=FuseBackboneInceptionResnetV2(input_channels_num=3),\n",
    "    heads=[\n",
    "        FuseHeadGlobalPoolingClassifier(head_name='head_0',\n",
    "                                        dropout_rate=0.5,\n",
    "                                        conv_inputs=[('model.backbone_features', 384)],\n",
    "                                        tabular_data_inputs=[(\"data.input.clinical.all\", 11)],\n",
    "                                        layers_description=(256,),\n",
    "                                        tabular_layers_description=(128,),\n",
    "                                        num_classes=8,\n",
    "                                        pooling=\"avg\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Strart a training process\n",
    "manager = FuseManagerDefault(output_model_dir=\"model_dir\", force_reset=True)\n",
    "# Providing the objects required for the training process.\n",
    "manager.set_objects(net=model,\n",
    "                    optimizer=optimizer,\n",
    "                    losses=losses,\n",
    "                    metrics=metrics,\n",
    "                    best_epoch_source=best_epoch_source,\n",
    "                    lr_scheduler=scheduler,\n",
    "                    callbacks={},\n",
    "                    train_params={'num_epochs': 2},\n",
    "                    output_model_dir=\"model_dir\")\n",
    "\n",
    "# Start training\n",
    "manager.train(train_dataloader=train_dl,\n",
    "                validation_dataloader=valid_dl)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imaging and Tabular data - concatenate directly with the image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Generate Data\n",
    "def post_cache_processing_clinical_pad_to_image(sample_dict: dict) -> dict:\n",
    "    sample_dict = post_cache_processing_clinical_encoding(sample_dict)\n",
    "    clinical_encodging = FuseUtilsHierarchicalDict.get(sample_dict, \"data.input.clinical.all\")\n",
    "    image = FuseUtilsHierarchicalDict.get(sample_dict, \"data.input.image\")\n",
    "\n",
    "    clinical_data_spatial = clinical_encodging.reshape((clinical_encodging.shape + (1,1))).repeat((1,) + image.shape[1:]) # repeat and reshape to [num_features, H, W]\n",
    "    image = torch.cat((image, clinical_data_spatial), dim=0) # concat to get [num_features + 3, H, W]\n",
    "\n",
    "    FuseUtilsHierarchicalDict.set(sample_dict, \"data.input.image\", image)\n",
    "\n",
    "    return sample_dict\n",
    "\n",
    "train_dl, valid_dl = isic_2019_dataset(size=400, reset_cache=False, post_cache_processing_func=post_cache_processing_clinical_pad_to_image)\n",
    "\n",
    "### Define model\n",
    "model = FuseModelDefault(\n",
    "    conv_inputs=(('data.input.image', 14),),\n",
    "    backbone=FuseBackboneInceptionResnetV2(input_channels_num=14),\n",
    "    heads=[\n",
    "        FuseHeadGlobalPoolingClassifier(head_name='head_0',\n",
    "                                        dropout_rate=0.5,\n",
    "                                        conv_inputs=[('model.backbone_features', 384)],\n",
    "                                        layers_description=(256,),\n",
    "                                        num_classes=8,\n",
    "                                        pooling=\"avg\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "### Strart a training process\n",
    "manager = FuseManagerDefault(output_model_dir=\"model_dir\", force_reset=True)\n",
    "# Providing the objects required for the training process.\n",
    "manager.set_objects(net=model,\n",
    "                    optimizer=optimizer,\n",
    "                    losses=losses,\n",
    "                    metrics=metrics,\n",
    "                    best_epoch_source=best_epoch_source,\n",
    "                    lr_scheduler=scheduler,\n",
    "                    callbacks={},\n",
    "                    train_params={'num_epochs': 2},\n",
    "                    output_model_dir=\"model_dir\")\n",
    "\n",
    "# Start training\n",
    "manager.train(train_dataloader=train_dl,\n",
    "                validation_dataloader=valid_dl)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "19223a2c7bb0f60930d3f6dd8707dc3d3ac264a621090aaf265f0cbe95014992"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('public-fuse': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}