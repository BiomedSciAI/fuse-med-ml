from collections import OrderedDict
import pathlib
from fuse.utils.utils_logger import fuse_logger_start
import os
import sys
# add parent directory to path, so that 'baseline' folder is treated as a module
sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
from fuse_examples.classification.knight.baseline.dataset import knight_dataset
import pandas as pd
from fuse.models.model_default import FuseModelDefault
from fuse.models.backbones.backbone_resnet_3d import FuseBackboneResnet3D
from fuse.models.heads.head_3D_classifier import FuseHead3dClassifier
from fuse.losses.loss_default import FuseLossDefault
import torch.nn.functional as F
import torch.nn as nn
from fuse.eval.metrics.classification.metrics_classification_common import MetricAUCROC, MetricAccuracy, MetricConfusion
from fuse.eval.metrics.classification.metrics_thresholding_common import MetricApplyThresholds
import torch.optim as optim
from fuse.managers.manager_default import FuseManagerDefault
from fuse.managers.callbacks.callback_tensorboard import FuseTensorboardCallback
from fuse.managers.callbacks.callback_metric_statistics import FuseMetricStatisticsCallback
import fuse.utils.gpu as FuseUtilsGPU
from fuse.utils.rand.seed import Seed
import logging
import time
from fuse.utils.dl.checkpoint import FuseCheckpoint
import gzip
import pickle

def update_parameters_by_task(data_path,task_num):
    parameters = {}
    parameters['knight_data'] = data_path
    parameters['dir_path'] = pathlib.Path(__file__).parent.resolve()
    parameters['force_gpus'] = None #specify the GPU indices you want to use
    parameters['batch_size'] = 2
    parameters['resize_to'] = (256, 256, 64)
    parameters['print_and_visualize'] = False

    if task_num == 1:
        parameters['num_epochs'] = 100
        parameters['num_classes'] = 2
        parameters['learning_rate'] = 1e-5
        parameters['imaging_dropout'] = 0.5
        parameters['target_name'] ='data.gt.gt_global.task_1_label'
        parameters['target_metric'] = 'metrics.auc'

        parameters['task_num'] = 1
        parameters['knight_results_path'] = root_path + '/task_1_model_example/'
        parameters['resume_checkpoint_filename'] = None
        parameters['knight_cache'] = root_path + 'task_1_cache'
        parameters['init_backbone'] = False


    elif task_num == 2:
        parameters['num_epochs'] = 150
        parameters['num_classes'] = 5
        parameters['learning_rate'] = 1e-4
        parameters['imaging_dropout'] = 0.7
        parameters['target_name']='data.gt.gt_global.task_2_label'
        parameters['target_metric']='metrics.auc'

        parameters['task_num'] = 2
        parameters['knight_results_path'] = root_path + '/task_2_model_example/'
        parameters['resume_checkpoint_filename'] = root_path + '/task_1_model_example/checkpoint_best_0_epoch.pth'
        parameters['knight_cache'] = root_path + 'task_2_cache'
        parameters['init_backbone'] = True

    return parameters

# Knight model
def run_train(parameters):

    # read train/val splits file. for convenience, we use the one
    # auto-generated by the nnU-Net framework for the KiTS21 data
    splits = pd.read_pickle(os.path.join(parameters['dir_path'], 'splits_final.pkl'))
    # For this example, we use split 0 out of the 5 available cross validation splits
    split = splits[0]

    # read environment variables for data, cache and results locations
    data_path = parameters['knight_data']
    cache_path = parameters['knight_cache']
    results_path = parameters['knight_results_path']


    ## Basic settings:
    ##############################################################################
    # create model results dir:
    # we use a time stamp in model directory name, to prevent re-writing
    # timestr = time.strftime("%Y%m%d-%H%M%S")
    # model_dir = os.path.join(results_path, timestr)
    model_dir = results_path
    if not os.path.isdir(model_dir):
        os.makedirs(model_dir)

    # start logger
    fuse_logger_start(output_path=model_dir, console_verbose_level=logging.INFO)
    print("Done")

    # set constant seed for reproducibility.
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ":4096:8"  # required for pytorch deterministic mode
    rand_gen = Seed.set_seed(1235, deterministic_mode=True)

    # select gpus
    FuseUtilsGPU.choose_and_enable_multiple_gpus(1, force_gpus=parameters['force_gpus'])

    ## FuseMedML dataset preparation
    ##############################################################################

    train_dl, valid_dl, _, _, _, _ = knight_dataset(data_dir=data_path, cache_dir=cache_path, split=split, \
                                                    reset_cache=False, rand_gen=rand_gen, batch_size=parameters['batch_size'],
                                                    resize_to=parameters['resize_to'], \
                                                    task_num=parameters['task_num'], target_name=parameters['target_name'],
                                                    num_classes=parameters['num_classes'])

    ## Simple data visualizations/analysis:
    ##############################################################################

    if parameters['print_and_visualize']:
        # an example of printing a sample from the data:
        sample_index = 10
        # print(train_dl.dataset[sample_index]['data']['input']['clinical']['all'])
        print(train_dl.dataset[sample_index])

        # print a summary of the label distribution:
        # print(train_dl.dataset.summary(["data.gt.gt_global.task_1_label"]))
        # print(valid_dl.dataset.summary(["data.gt.gt_global.task_1_label"]))

        # visualize a sample
        # this will only do anything if a matplotlib gui backend is set appropriately, or in "notebook mode"
        train_dl.dataset.visualize(sample_index)

        # visualize a sample with augmentations applied:
        train_dl.dataset.visualize_augmentation(sample_index)

    ## Model definition
    ##############################################################################

    backbone = FuseBackboneResnet3D(in_channels=1)
    conv_inputs = [('model.backbone_features', 512)]
    append_features = None # do not append clinical features in the DL model


    model = FuseModelDefault(
        conv_inputs=(('data.input.image', 1),),
        backbone=backbone,
        heads=[
            FuseHead3dClassifier(head_name='head_0',
                                 conv_inputs=conv_inputs,
                                 dropout_rate=parameters['imaging_dropout'],
                                 num_classes=parameters['num_classes'],
                                 append_features=append_features,
                                 append_layers_description=(256, 128),
                                 ),
        ]
    )

    # load weights backbone weights
    # #load backbone weights
    if parameters['init_backbone']:
        checkpoint_file = parameters['resume_checkpoint_filename']
        net_state_dict_list = FuseCheckpoint.load_from_file(checkpoint_file)
        backbone_state = model.backbone.state_dict()
        state_dict = net_state_dict_list.net_state_dict
        for name, param in state_dict.items():
            print(name)
            if ('backbone' in name):
                target_name = name.split('backbone.')[1]
                backbone_state[target_name].copy_(param.data)

        model.backbone.load_state_dict(*[backbone_state], strict=True)
        parameters['resume_checkpoint_filename'] = None

    # Loss definition:
    ##############################################################################
    losses = {
        'cls_loss': FuseLossDefault(pred_name='model.logits.head_0', target_name=parameters['target_name'],
                                    callable=F.cross_entropy, weight=1.0)
    }

    # Metrics definition:
    ##############################################################################
    metrics = OrderedDict([
        ('op', MetricApplyThresholds(pred='model.output.head_0')),  # will apply argmax
        ('auc', MetricAUCROC(pred='model.output.head_0', target=parameters['target_name'])),
        ('accuracy', MetricAccuracy(pred='results:metrics.op.cls_pred', target=parameters['target_name'])),
        ('sensitivity',
         MetricConfusion(pred='results:metrics.op.cls_pred', target=parameters['target_name'], metrics=('sensitivity',))),

    ])

    best_epoch_source = {
        'source': parameters['target_metric'],  # can be any key from losses or metrics dictionaries
        'optimization': 'max',  # can be either min/max
    }

    # Optimizer definition:
    ##############################################################################
    optimizer = optim.Adam(model.parameters(), lr=parameters['learning_rate'],
                           weight_decay=0.001)

    # Scheduler definition:
    ##############################################################################
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)

    ## Training
    ##############################################################################

    # set tensorboard callback
    callbacks = {
        FuseTensorboardCallback(model_dir=model_dir),  # save statistics for tensorboard
        FuseMetricStatisticsCallback(output_path=model_dir + "/metrics.csv"),  # save statistics a csv file

    }
    manager = FuseManagerDefault(output_model_dir=model_dir, force_reset=True)
    manager.set_objects(net=model,
                        optimizer=optimizer,
                        losses=losses,
                        metrics=metrics,
                        best_epoch_source=best_epoch_source,
                        lr_scheduler=scheduler,
                        callbacks=callbacks,
                        train_params={'num_epochs': parameters['num_epochs'], 'lr_sch_target': 'train.losses.total_loss'},
                        # 'lr_sch_target': 'validation.metrics.auc.macro_avg'
                        output_model_dir=model_dir)

    ## Continue training
    if parameters['resume_checkpoint_filename'] is not None:
        # Loading the checkpoint including model weights, learning rate, and epoch_index.
        manager.load_checkpoint(checkpoint=parameters['resume_checkpoint_filename'], mode='train')

    print('Training...')
    manager.train(train_dataloader=train_dl,
                  validation_dataloader=valid_dl)

def train_example(root_path,data_path):
    # Data sources to use in model. Set {'imaging': True, 'clinical': False} for imaging only setting,
    # and vice versa, or set both to True to use both.
    # allocate gpus
    # uncomment if you want to use specific gpus instead of automatically looking for free ones

    task_loop = [1,2]

    for task_num in task_loop:
        parameters = update_parameters_by_task(data_path, task_num)
        run_train(parameters)
    return parameters


def run_infer(parameters,train_file,infer_file):
    cache_path = parameters['knight_cache']
    splits = pd.read_pickle(os.path.join(parameters['dir_path'], 'splits_final.pkl'))
    # For this example, we use split 0 out of the 5 available cross validation splits
    split = splits[0]

    #### Logger
    fuse_logger_start(infer_path, console_verbose_level=logging.INFO)
    lgr = logging.getLogger('Fuse')
    lgr.info('Fuse Inference', {'attrs': ['bold', 'underline']})

    lgr.info(f'infer_filename={infer_file}',{'color': 'magenta'})

    # Create data source:
    train_dataloader, infer_dataloader, _, _, _, _ = knight_dataset(data_dir=data_path, cache_dir=cache_path, split=split, \
                                                    reset_cache=False, rand_gen=None,
                                                    batch_size=parameters['batch_size'],
                                                    resize_to=parameters['resize_to'], \
                                                    task_num=parameters['task_num'],
                                                    target_name=parameters['target_name'],
                                                    num_classes=parameters['num_classes'])

    lgr.info(f'Test Data: Done', {'attrs': 'bold'})

    #### Manager for inference
    manager = FuseManagerDefault()
    # extract just the global classification per sample and save to a file
    output_columns = ['model.output.head_0','data.gt.gt_global.task_2_label']
    results_train = manager.infer(data_loader=train_dataloader,
                  input_model_dir=parameters['knight_results_path'],
                  checkpoint='best',
                  output_columns=output_columns,
                  output_file_name=train_file)
    manager = FuseManagerDefault()
    results_infer = manager.infer(data_loader=infer_dataloader,
                  input_model_dir=parameters['knight_results_path'],
                  checkpoint='best',
                  output_columns=output_columns,
                  output_file_name=infer_file)
    return

def infer_example(parameters,train_file, infer_file):
    # allocate gpus
    NUM_GPUS = 1
    force_gpus = None #None
    FuseUtilsGPU.choose_and_enable_multiple_gpus(NUM_GPUS, force_gpus=force_gpus)
    run_infer(parameters,train_file,infer_file)

def save_results_csv(infer_file, task_num, csv_path=None):
    if csv_path is None:
        csv_path = os.path.join(infer_path,'infer_validation.csv')
    with gzip.open(infer_file, 'rb') as pickle_file:
        results_df = pickle.load(pickle_file)
    if task_num == 1:
        cols = ['pred_negative', 'pred_positive']
    else:
        cols = ['pred_classA', 'pred_classB', 'pred_classC','pred_classD','pred_classE']
    results_df[cols] = pd.DataFrame(results_df['model.output.head_0'].tolist(), index=results_df.index)
    results_df.drop('model.output.head_0', axis=1, inplace=True)
    results_df.to_csv(csv_path)





if __name__ == "__main__":

    ##########################################
    # CNN training
    # 1) pretrain model using binary task
    # 2) Initialize weights and re-train on multi-class task
    ##########################################
    root_path = 'knight_for_virtual_biopsy'
    data_path = '/projects/msieve/MedicalSieve/PatientData/KNIGHT/'
    parameters = train_example(root_path,data_path)
    #
    # # ##########################################
    # # # Infer and create csv file of prediction per class
    # # ##########################################
    infer_path =root_path+'_infer/' # creates folder knight_for_virtual_biopsy_infer where all csv files will be saved
    train_file = os.path.join(infer_path,'infer_train.pickle.gz')
    infer_file = os.path.join(infer_path,'infer_validation.pickle.gz')

    parameters = update_parameters_by_task(data_path,task_num=1)
    infer_example(parameters,train_file,infer_file)

    save_results_csv(train_file,1, os.path.join(infer_path,'infer_train_task1.csv'))
    save_results_csv(infer_file,1, os.path.join(infer_path,'infer_validation_task1.csv'))


    parameters = update_parameters_by_task(data_path,task_num=2)
    infer_example(parameters,train_file,infer_file)

    save_results_csv(train_file,2, os.path.join(infer_path,'infer_train_task2.csv'))
    save_results_csv(infer_file,2, os.path.join(infer_path,'infer_validation_task2.csv'))
