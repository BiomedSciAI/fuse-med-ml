experiment : try_dino
model_dir : /dccstor/mm_hcls2/oai/results/
csv_path : /dccstor/mm_hcls2/oai/code/data/relevant_dicoms.csv #fill
clearml_project_name : OAI/Dino

resize_to : [224,224]
batch_size : 4
batch_size_eval : 8
precision : 32
train_folds : [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18]
val_folds : [19]
reset_cache : False
debug : False

n_workers : 16
backbone : unet3d
suprem_weights : /dccstor/mm_hcls2/oai/code/archs/weights/supervised_suprem_unet_2100.pth # if pretrained=True a path need to be filled. can download from https://github.com/MrGiovanni/SuPreM
resume_training_from : null #resume training from checkpoint

projection: 2048

momentum_teacher : 0.9995
n_crops : 4 # 4, orig Dino 6 local 2 global
out_dim : 512
cls_dim: 256
n_epochs : 70
learning_rate :  0.002
cls_learning_rate : 0.0001
clip_grad : 3.0
warmup_teacher_temp : 0.04
warmup_teacher_temp_epochs : 0
teacher_temp : 0.04
student_temp : 0.1
weight_decay : 0.04
weight_decay_end : 0.4 #Final value of the weight decay. We use a cosine schedule for WD and using a larger decay by the end of training improves performance for ViTs.
norm_last_layer : True
use_bn_in_head : False #Whether to use batch normalizations in projection head (Default: False)
freeze_last_layer : 1 #Number of epochs during which we keep the output layer fixed. Typically doing so during the first epoch helps training. Try increasing this value if the loss does not decrease.

clearml : True
